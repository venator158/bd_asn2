from pyspark.sql import SparkSession
from pyspark.sql.functions import avg, when, col

# Initialize Spark
spark = SparkSession.builder.appName("CPU_MEM_Alerts").getOrCreate()

# ✅ Hardcoded thresholds
cpu_threshold = 81.73
mem_threshold = 78.31
net_in_threshold = 5534.73
disk_io_threshold = 1737.83   # (not used in this script yet, but kept for later use)

# Read CPU and Memory CSVs (generated by Consumer 1)
cpu_df = spark.read.csv("/home/pes2ug23cs589/cpu_data.csv", header=True, inferSchema=True)
mem_df = spark.read.csv("/home/pes2ug23cs589/mem_data.csv", header=True, inferSchema=True)

# Join CPU and Memory data on server_id
# If timestamps are present and need match, include them in join.
avg_df = (
    cpu_df.join(mem_df, "server_id")
          .groupBy("server_id")
          .agg(avg("cpu_pct").alias("avg_cpu"), avg("mem").alias("avg_mem"))
)

# Apply conditions for alerts
alerts_df = avg_df.withColumn(
    "Alert",
    when((col("avg_cpu") > cpu_threshold) & (col("avg_mem") > mem_threshold), "High CPU + Memory stress")
    .when((col("avg_cpu") > cpu_threshold) & (col("avg_mem") <= mem_threshold), "CPU spike suspected")
    .when((col("avg_mem") > mem_threshold) & (col("avg_cpu") <= cpu_threshold), "Memory saturation suspected")
    .otherwise("Normal")
)

# Show result in console
alerts_df.show(truncate=False)

# Save to CSV (team file)
alerts_df.coalesce(1).write.csv("team_17_NO_CPU_MEM.csv", header=True, mode="overwrite")

print("✅ Alerts generated and saved to team_17_NO_CPU_MEM.csv")
